{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 DoF Pose from Semantic Keypoints #\n",
    "\n",
    "The following is an implementation of https://www.seas.upenn.edu/~pavlakos/projects/object3d/\n",
    "Find the associated tutorial at:https://medium.com/@vaishakvk/geometric-deep-learning-for-pose-estimation-6af45da05922\n",
    "\n",
    "This is an adaptation of code used in Prof.Kostas Daniilidis' course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mines/mines_ws\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/mines_ws\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import json, random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from src.keypoints_detection.hourglass import hg\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import os\n",
    "#from os import join\n",
    "from time import time\n",
    "import numpy as np\n",
    "import cv2 \n",
    "\n",
    "os.makedirs('./output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint Localization ##\n",
    "\n",
    "First, we will create the dataset class and a few transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_dir='KeypointTrainingData', is_train=True, transform=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        file_name = \"frame_data_train.json\" if is_train else \"frame_data_test.json\"\n",
    "\n",
    "        path = f\"{dataset_dir}/{file_name}\"\n",
    "        with open(path, 'r') as f:\n",
    "            self.data = dict(json.load(f))\n",
    "\n",
    "        self.img_names = list(self.data.keys())\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.img_names[idx]\n",
    "\n",
    "        image_dir = self.data[image_name][\"img_dir\"]\n",
    "\n",
    "        image = cv2.imread(f\"{image_dir}/Images/{image_name}.png\")\n",
    "        # bb = [0,0,image.shape[1], image.shape[0]]\n",
    "        bb = self.data[image_name][\"bbox\"]\n",
    "        # image = Image.fromarray((image[:,:,:3]*255).astype(np.uint8))\n",
    "        image = Image.fromarray(image)\n",
    "        # bb = self.data[image_name][\"bbox\"]\n",
    "\n",
    "        keypoints = self.data[image_name][\"keypoints\"]\n",
    "        keypoints = np.array(keypoints)\n",
    "        # print(keypoints)\n",
    "        item = {'image': image, 'bb': bb, 'keypoints': keypoints}\n",
    "        # print(keypoints.shape)\n",
    "        if self.transform is not None:\n",
    "            item = self.transform(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformations\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def generate_heatmap(heatmap, pt, sigma):\n",
    "    heatmap[int(pt[1])][int(pt[0])] = 1\n",
    "    heatmap = cv2.GaussianBlur(heatmap, sigma, 0)\n",
    "    am = np.amax(heatmap)\n",
    "    heatmap /= am\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "class Blurr:\n",
    "    def __init__(self, radius) -> None:\n",
    "        import time\n",
    "        self.radius = radius\n",
    "        random.seed(time.time())\n",
    "\n",
    "    def __call__(self, sample) -> Any:\n",
    "        img = sample['image']\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(2,4)))\n",
    "        enhancer = ImageEnhance.Brightness(img)\n",
    "        img = enhancer.enhance(random.uniform(0.5, 1))\n",
    "        sample['image'] = img\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CropAndPad:\n",
    "\n",
    "    def __init__(self, out_size=(256,256)):\n",
    "        self.out_size = out_size[::-1]\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, bb = sample['image'], sample['bb']\n",
    "        img_size = image.size\n",
    "        min_x, min_y = bb[:2]\n",
    "        max_x, max_y = bb[2:]\n",
    "        center_x = (min_x + max_x) / 2\n",
    "        center_y = (min_y + max_y) / 2\n",
    "        width = max([max_x-min_x, max_y-min_y])\n",
    "        min_x = int(center_x) - int(width)//2\n",
    "        min_y = int(center_y) - int(width)//2\n",
    "        max_x = int(center_x) + int(width)//2\n",
    "        max_y = int(center_y) + int(width)//2\n",
    "        sample['image'] = image.crop(box=(min_x,min_y,max_x,max_y))\n",
    "        sample['orig_image'] = image\n",
    "        sample['center'] = np.array([center_x, center_y], dtype=np.float32)\n",
    "        sample['min'] = np.array([min_x, min_y], dtype=np.float32)\n",
    "        sample['scale'] = np.array([width/self.out_size[0]], dtype=np.float32)\n",
    "        sample['width'] = width\n",
    "        if width != self.out_size[0]:\n",
    "            sample['image'] = sample['image'].resize(self.out_size)\n",
    "        if 'mask' in sample:\n",
    "            sample['mask'] = sample['mask'].crop(box=(min_x,min_y,max_x,max_y)).resize(self.out_size)\n",
    "        if 'keypoints' in sample:\n",
    "            keypoints = sample['keypoints']\n",
    "            for i in range(keypoints.shape[0]):\n",
    "                # if keypoints[i,2] != 0:\n",
    "                if keypoints[i,0] < min_x or keypoints[i,0] > max_x or keypoints[i,1] < min_y or keypoints[i,1] > max_y:\n",
    "                    keypoints[i,:] = [0,0,0]\n",
    "                else:\n",
    "                    keypoints[i,:2] = (keypoints[i,:2]-np.array([min_x, min_y]))*self.out_size/width\n",
    "        sample['keypoints'] = keypoints\n",
    "        sample.pop('bb')\n",
    "        return sample\n",
    "\n",
    "# Convert keypoint locations to heatmaps\n",
    "class LocsToHeatmaps:\n",
    "\n",
    "    def __init__(self, img_size=(256,256), out_size=(64,64), sigma=1):\n",
    "        self.img_size = img_size\n",
    "        self.out_size = out_size\n",
    "        self.x_scale = 1.0 * out_size[0]/img_size[0]\n",
    "        self.y_scale = 1.0 * out_size[1]/img_size[1]\n",
    "        self.sigma=sigma\n",
    "        x = np.arange(0, out_size[1], dtype=np.float32)\n",
    "        y = np.arange(0, out_size[0], dtype=np.float32)\n",
    "        self.yg, self.xg = np.meshgrid(y,x, indexing='ij')\n",
    "        return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sigma = 7\n",
    "        gaussian_hm = np.zeros((self.out_size[0], self.out_size[1], sample['keypoints'].shape[0]))\n",
    "        for i,keypoint in enumerate(sample['keypoints']):\n",
    "            if keypoint[2] != 0:\n",
    "                gaussian_hm[:,:,i] = generate_heatmap(gaussian_hm[:,:,i], tuple(keypoint[:-1].astype(np.int64) * self.x_scale), (sigma, sigma))\n",
    "        sample['keypoint_locs'] = sample['keypoints'][:,:2]\n",
    "        sample['visible_keypoints'] = sample['keypoints'][:,2]\n",
    "        sample['keypoint_heatmaps'] = gaussian_hm\n",
    "        return sample\n",
    "\n",
    "# Convert numpy arrays to Tensor objects\n",
    "# Permute the image dimensions\n",
    "class ToTensor:\n",
    "\n",
    "    def __init__(self, downsample_mask=False):\n",
    "        self.tt = transforms.ToTensor()\n",
    "        self.downsample_mask=downsample_mask\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample['image'] = self.tt(sample['image'])\n",
    "        if 'orig_image' in sample:\n",
    "            sample['orig_image'] = self.tt(sample['orig_image'])\n",
    "        if 'mask' in sample:\n",
    "            if self.downsample_mask:\n",
    "                sample['mask'] = self.tt(sample['mask'].resize((64,64), Image.ANTIALIAS))\n",
    "            else:\n",
    "                sample['mask'] = self.tt(sample['mask'])\n",
    "        if 'in_mask' in sample:\n",
    "            sample['in_mask'] = self.tt(sample['in_mask'])\n",
    "            # sample['in_mask'] = sample['in_mask'].unsqueeze(0)\n",
    "        if 'keypoint_heatmaps' in sample:\n",
    "            sample['keypoint_heatmaps'] =\\\n",
    "                torch.from_numpy(sample['keypoint_heatmaps'].astype(np.float32).transpose(2,0,1))\n",
    "            sample['keypoint_locs'] =\\\n",
    "                torch.from_numpy(sample['keypoint_locs'].astype(np.float32))\n",
    "            sample['visible_keypoints'] =\\\n",
    "                torch.from_numpy(sample['visible_keypoints'].astype(np.float32))\n",
    "        return sample\n",
    "\n",
    "class Normalize:\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample['image'] = 2*(sample['image']-0.5)\n",
    "        if 'in_mask' in sample:\n",
    "            sample['in_mask'] = 2*(sample['in_mask']-0.5)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, dataset_dir, learning_rate, model_path: str = None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        train_transform_list = [Blurr(2.2), CropAndPad(out_size=(256, 256)),LocsToHeatmaps(out_size=(64, 64)),ToTensor(),Normalize()]\n",
    "        test_transform_list = [CropAndPad(out_size=(256, 256)),LocsToHeatmaps(out_size=(64, 64)),ToTensor(),Normalize()]\n",
    "        self.train_ds = Dataset(is_train=True, transform=transforms.Compose(train_transform_list), dataset_dir=dataset_dir)\n",
    "        self.test_ds = Dataset(is_train=False, transform=transforms.Compose(test_transform_list), dataset_dir=dataset_dir)\n",
    "\n",
    "        self.model = hg(num_stacks=1, num_blocks=1, num_classes=10).to(self.device)\n",
    "\n",
    "        if model_path is not None:\n",
    "            print(\"loading model from checkpoint\")\n",
    "            try:\n",
    "                checkpoint = torch.load(model_path)\n",
    "                self.model.load_state_dict(checkpoint['model'])\n",
    "            except:\n",
    "                print(\"No checkpoint loaded\")\n",
    "        else:\n",
    "            print(\"Not using checkpointed model\")\n",
    "        # define loss function and optimizer\n",
    "        self.heatmap_loss = torch.nn.MSELoss().to(self.device) # for Global loss\n",
    "        self.optimizer = torch.optim.RMSprop(self.model.parameters(),\n",
    "                                             lr = learning_rate)#2.5e-4)\n",
    "        self.train_data_loader = DataLoader(self.train_ds, batch_size=16,\n",
    "                                            num_workers=8,\n",
    "                                            pin_memory=True,\n",
    "                                            shuffle=True)\n",
    "        self.test_data_loader = DataLoader(self.test_ds, batch_size=32,\n",
    "                                           num_workers=8,\n",
    "                                           pin_memory=True,\n",
    "                                           shuffle=True)\n",
    "\n",
    "        self.summary_iters = []\n",
    "        self.losses = []\n",
    "        self.pcks = []\n",
    "        self.running_losses = []\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        self.total_step_count = 0\n",
    "        start_time = time()\n",
    "        for epoch in range(1,num_epochs+1):\n",
    "\n",
    "            print(\"Epoch%d/%d\"%\n",
    "                    (epoch,num_epochs), end=\"\\r\")\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "            for step, batch in enumerate(self.train_data_loader):\n",
    "                self.model.train()\n",
    "                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k,v in batch.items()}\n",
    "                self.optimizer.zero_grad()\n",
    "                pred_heatmap_list = self.model(batch['image'])\n",
    "                loss = self.heatmap_loss(pred_heatmap_list[-1], batch['keypoint_heatmaps'])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()                                          \n",
    "                \n",
    "                self.total_step_count += 1\n",
    "\n",
    "                running_loss += loss.detach()\n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    print(f\"running loss at step {step} = \\t{running_loss}\", end=\"\\r\")\n",
    "            \n",
    "            self.running_losses.append(running_loss)\n",
    "            print(f\"Epoch {epoch} / {num_epochs} total Loss\\t\")\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Savig Checkpoint at epoch {epoch}\")\n",
    "                checkpoint = {'model': self.model.state_dict()}\n",
    "                torch.save(checkpoint, './output/model_checkpoint.pt')\n",
    "                \n",
    "        checkpoint = {'model': self.model.state_dict()}\n",
    "        torch.save(checkpoint, './output/model_checkpoint.pt')\n",
    "\n",
    "    def save(self):\n",
    "        checkpoint = {'model': self.model.state_dict()}\n",
    "        print(\"saving checkpoint\")\n",
    "        torch.save(checkpoint, './output/model_checkpoint_2.pt')\n",
    "        torch.save(self.model, './output/full_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/NewData FrameData.json\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"./data/NewData\"\n",
    "%run ~/mines_ws/src/data_collection/preprocessing/fix_json.py {data_directory} FrameData.json\n",
    "data_directory += \"/accumulated_data\"\n",
    "\n",
    "model_path = \"./models/model_checkpoint.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using checkpointed model\n",
      "Epoch 1 / 50 total Loss\t= \t0.8574088215827942\n",
      "Epoch2/50\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f637564615f636f6e7461696e6572227d/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(dataset_dir\u001b[39m=\u001b[39mdata_directory, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.00005\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f637564615f636f6e7461696e6572227d/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(num_epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[1;32m/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f637564615f636f6e7461696e6572227d/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f637564615f636f6e7461696e6572227d/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m         (epoch,num_epochs), end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f637564615f636f6e7461696e6572227d/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f637564615f636f6e7461696e6572227d/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f637564615f636f6e7461696e6572227d/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f637564615f636f6e7461696e6572227d/home/mines/mines_ws/src/notebooks/Keypoint_Training.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     batch \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m v \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1285\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dataset_dir=data_directory, learning_rate=0.00005)\n",
    "trainer.train(num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "trainer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
